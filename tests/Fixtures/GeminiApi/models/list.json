{
    "statusCode": 200,
    "headers": {
        "Content-Type": "application\/json; charset=UTF-8",
        "Vary": [
            "X-Origin",
            "Referer",
            "Origin,Accept-Encoding"
        ],
        "Date": "Sat, 27 Jul 2024 14:53:21 GMT",
        "Server": "scaffolding on HTTPServer2",
        "Cache-Control": "private",
        "X-XSS-Protection": "0",
        "X-Frame-Options": "SAMEORIGIN",
        "X-Content-Type-Options": "nosniff",
        "Server-Timing": "gfet4t7; dur=1430",
        "Alt-Svc": "h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000",
        "Accept-Ranges": "none",
        "Transfer-Encoding": "chunked"
    },
    "data": "{\n  \"models\": [\n    {\n      \"name\": \"models\/chat-bison-001\",\n      \"version\": \"001\",\n      \"displayName\": \"PaLM 2 Chat (Legacy)\",\n      \"description\": \"A legacy text-only model optimized for chat conversations\",\n      \"inputTokenLimit\": 4096,\n      \"outputTokenLimit\": 1024,\n      \"supportedGenerationMethods\": [\n        \"generateMessage\",\n        \"countMessageTokens\"\n      ],\n      \"temperature\": 0.25,\n      \"topP\": 0.95,\n      \"topK\": 40\n    },\n    {\n      \"name\": \"models\/text-bison-001\",\n      \"version\": \"001\",\n      \"displayName\": \"PaLM 2 (Legacy)\",\n      \"description\": \"A legacy model that understands text and generates text as an output\",\n      \"inputTokenLimit\": 8196,\n      \"outputTokenLimit\": 1024,\n      \"supportedGenerationMethods\": [\n        \"generateText\",\n        \"countTextTokens\",\n        \"createTunedTextModel\"\n      ],\n      \"temperature\": 0.7,\n      \"topP\": 0.95,\n      \"topK\": 40\n    },\n    {\n      \"name\": \"models\/embedding-gecko-001\",\n      \"version\": \"001\",\n      \"displayName\": \"Embedding Gecko\",\n      \"description\": \"Obtain a distributed representation of a text.\",\n      \"inputTokenLimit\": 1024,\n      \"outputTokenLimit\": 1,\n      \"supportedGenerationMethods\": [\n        \"embedText\",\n        \"countTextTokens\"\n      ]\n    },\n    {\n      \"name\": \"models\/gemini-1.0-pro-latest\",\n      \"version\": \"001\",\n      \"displayName\": \"Gemini 1.0 Pro Latest\",\n      \"description\": \"The best model for scaling across a wide range of tasks. This is the latest model.\",\n      \"inputTokenLimit\": 30720,\n      \"outputTokenLimit\": 2048,\n      \"supportedGenerationMethods\": [\n        \"generateContent\",\n        \"countTokens\"\n      ],\n      \"temperature\": 0.9,\n      \"topP\": 1\n    },\n    {\n      \"name\": \"models\/gemini-1.0-pro\",\n      \"version\": \"001\",\n      \"displayName\": \"Gemini 1.0 Pro\",\n      \"description\": \"The best model for scaling across a wide range of tasks\",\n      \"inputTokenLimit\": 30720,\n      \"outputTokenLimit\": 2048,\n      \"supportedGenerationMethods\": [\n        \"generateContent\",\n        \"countTokens\"\n      ],\n      \"temperature\": 0.9,\n      \"topP\": 1\n    },\n    {\n      \"name\": \"models\/gemini-pro\",\n      \"version\": \"001\",\n      \"displayName\": \"Gemini 1.0 Pro\",\n      \"description\": \"The best model for scaling across a wide range of tasks\",\n      \"inputTokenLimit\": 30720,\n      \"outputTokenLimit\": 2048,\n      \"supportedGenerationMethods\": [\n        \"generateContent\",\n        \"countTokens\"\n      ],\n      \"temperature\": 0.9,\n      \"topP\": 1\n    },\n    {\n      \"name\": \"models\/gemini-1.0-pro-001\",\n      \"version\": \"001\",\n      \"displayName\": \"Gemini 1.0 Pro 001 (Tuning)\",\n      \"description\": \"The best model for scaling across a wide range of tasks. This is a stable model that supports tuning.\",\n      \"inputTokenLimit\": 30720,\n      \"outputTokenLimit\": 2048,\n      \"supportedGenerationMethods\": [\n        \"generateContent\",\n        \"countTokens\",\n        \"createTunedModel\"\n      ],\n      \"temperature\": 0.9,\n      \"topP\": 1\n    },\n    {\n      \"name\": \"models\/gemini-1.0-pro-vision-latest\",\n      \"version\": \"001\",\n      \"displayName\": \"Gemini 1.0 Pro Vision\",\n      \"description\": \"The best image understanding model to handle a broad range of applications\",\n      \"inputTokenLimit\": 12288,\n      \"outputTokenLimit\": 4096,\n      \"supportedGenerationMethods\": [\n        \"generateContent\",\n        \"countTokens\"\n      ],\n      \"temperature\": 0.4,\n      \"topP\": 1,\n      \"topK\": 32\n    },\n    {\n      \"name\": \"models\/gemini-pro-vision\",\n      \"version\": \"001\",\n      \"displayName\": \"Gemini 1.0 Pro Vision\",\n      \"description\": \"The best image understanding model to handle a broad range of applications\",\n      \"inputTokenLimit\": 12288,\n      \"outputTokenLimit\": 4096,\n      \"supportedGenerationMethods\": [\n        \"generateContent\",\n        \"countTokens\"\n      ],\n      \"temperature\": 0.4,\n      \"topP\": 1,\n      \"topK\": 32\n    },\n    {\n      \"name\": \"models\/gemini-1.5-pro-latest\",\n      \"version\": \"001\",\n      \"displayName\": \"Gemini 1.5 Pro Latest\",\n      \"description\": \"Mid-size multimodal model that supports up to 2 million tokens\",\n      \"inputTokenLimit\": 2097152,\n      \"outputTokenLimit\": 8192,\n      \"supportedGenerationMethods\": [\n        \"generateContent\",\n        \"countTokens\"\n      ],\n      \"temperature\": 1,\n      \"topP\": 0.95,\n      \"topK\": 64,\n      \"maxTemperature\": 2\n    },\n    {\n      \"name\": \"models\/gemini-1.5-pro-001\",\n      \"version\": \"001\",\n      \"displayName\": \"Gemini 1.5 Pro 001\",\n      \"description\": \"Mid-size multimodal model that supports up to 2 million tokens\",\n      \"inputTokenLimit\": 2097152,\n      \"outputTokenLimit\": 8192,\n      \"supportedGenerationMethods\": [\n        \"generateContent\",\n        \"countTokens\",\n        \"createCachedContent\"\n      ],\n      \"temperature\": 1,\n      \"topP\": 0.95,\n      \"topK\": 64,\n      \"maxTemperature\": 2\n    },\n    {\n      \"name\": \"models\/gemini-1.5-pro\",\n      \"version\": \"001\",\n      \"displayName\": \"Gemini 1.5 Pro\",\n      \"description\": \"Mid-size multimodal model that supports up to 2 million tokens\",\n      \"inputTokenLimit\": 2097152,\n      \"outputTokenLimit\": 8192,\n      \"supportedGenerationMethods\": [\n        \"generateContent\",\n        \"countTokens\"\n      ],\n      \"temperature\": 1,\n      \"topP\": 0.95,\n      \"topK\": 64,\n      \"maxTemperature\": 2\n    },\n    {\n      \"name\": \"models\/gemini-1.5-flash-latest\",\n      \"version\": \"001\",\n      \"displayName\": \"Gemini 1.5 Flash Latest\",\n      \"description\": \"Fast and versatile multimodal model for scaling across diverse tasks\",\n      \"inputTokenLimit\": 1048576,\n      \"outputTokenLimit\": 8192,\n      \"supportedGenerationMethods\": [\n        \"generateContent\",\n        \"countTokens\"\n      ],\n      \"temperature\": 1,\n      \"topP\": 0.95,\n      \"topK\": 64,\n      \"maxTemperature\": 2\n    },\n    {\n      \"name\": \"models\/gemini-1.5-flash-001\",\n      \"version\": \"001\",\n      \"displayName\": \"Gemini 1.5 Flash 001\",\n      \"description\": \"Fast and versatile multimodal model for scaling across diverse tasks\",\n      \"inputTokenLimit\": 1048576,\n      \"outputTokenLimit\": 8192,\n      \"supportedGenerationMethods\": [\n        \"generateContent\",\n        \"countTokens\",\n        \"createCachedContent\"\n      ],\n      \"temperature\": 1,\n      \"topP\": 0.95,\n      \"topK\": 64,\n      \"maxTemperature\": 2\n    },\n    {\n      \"name\": \"models\/gemini-1.5-flash\",\n      \"version\": \"001\",\n      \"displayName\": \"Gemini 1.5 Flash\",\n      \"description\": \"Fast and versatile multimodal model for scaling across diverse tasks\",\n      \"inputTokenLimit\": 1048576,\n      \"outputTokenLimit\": 8192,\n      \"supportedGenerationMethods\": [\n        \"generateContent\",\n        \"countTokens\"\n      ],\n      \"temperature\": 1,\n      \"topP\": 0.95,\n      \"topK\": 64,\n      \"maxTemperature\": 2\n    },\n    {\n      \"name\": \"models\/embedding-001\",\n      \"version\": \"001\",\n      \"displayName\": \"Embedding 001\",\n      \"description\": \"Obtain a distributed representation of a text.\",\n      \"inputTokenLimit\": 2048,\n      \"outputTokenLimit\": 1,\n      \"supportedGenerationMethods\": [\n        \"embedContent\"\n      ]\n    },\n    {\n      \"name\": \"models\/text-embedding-004\",\n      \"version\": \"004\",\n      \"displayName\": \"Text Embedding 004\",\n      \"description\": \"Obtain a distributed representation of a text.\",\n      \"inputTokenLimit\": 2048,\n      \"outputTokenLimit\": 1,\n      \"supportedGenerationMethods\": [\n        \"embedContent\"\n      ]\n    },\n    {\n      \"name\": \"models\/aqa\",\n      \"version\": \"001\",\n      \"displayName\": \"Model that performs Attributed Question Answering.\",\n      \"description\": \"Model trained to return answers to questions that are grounded in provided sources, along with estimating answerable probability.\",\n      \"inputTokenLimit\": 7168,\n      \"outputTokenLimit\": 1024,\n      \"supportedGenerationMethods\": [\n        \"generateAnswer\"\n      ],\n      \"temperature\": 0.2,\n      \"topP\": 1,\n      \"topK\": 40\n    }\n  ]\n}\n"
}